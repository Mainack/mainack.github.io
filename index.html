
<!DOCTYPE html>
<html lang="en">
  <head>
    <title> Mainack Mondal, Assiatant Professor, IIT Kgp</title>
    <meta name="robots" content="nofollow">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="generator" content="Bootply" />  
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Home page of Mainack Mondal" />
    <meta name="author" content="Mainack Mondal">	
	<script type="text/javascript">function toggle(id){var e = document.getElementById(id);if (e.style.display == '')e.style.display = 'none';else e.style.display = '';}</script>
    <!-- Bootstrap core CSS -->
    <link href="bootstrap/css/bootstrap.css" rel="stylesheet">
	<script>
		var shiftWindow = function() { scrollBy(0, -50) };
		if (location.hash) shiftWindow();
		window.addEventListener("hashchange", shiftWindow);	
	</script>
<!-- web analytics stuff below -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-23173897-3', 'auto');
ga('send', 'pageview');
</script>
	
  </head>
  
  <body>

    <nav class="navbar  navbar-inverse  navbar-fixed-top" role="navigation">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand glyphicon glyphicon-home" href="#"></a>
        </div>
	<!-- add more sections if needed later -->
        <div class="collapse navbar-collapse">
          <ul class="nav navbar-nav navbar-right">
            <!--<li class="active"><a href="#bio" target="_top">Bio</a></li>-->
            <!--<li><a href= "#bio" target="_top">Bio</a></li>  -->
            <li><a href="#updates" target="_top">Latest updates</a></li>         
            <li><a href="#research" target="_top">Research</a></li>
            <li ><a href="#publications" target="_top">Publications</a></li>            
            <li><a href="#softwaresdatasets" target="_top">Our systems/Datasets</a></li>     
            <!--<li><a href="#datasets" target="_top">Datasets</a></li>-->
            <li><a href="#teaching" target="_top">Teaching</a></li>  
            <li><a href="#services" target="_top">Professional Services</a></li>         
            <!--<li><a href="#cv" target="_top">CV</a></li>-->                   
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </nav>
    
    <div style="margin:63px;">
    	<div class="container-fluid">
			<div class=row>
                <div class="col-md-3">
					<div class="thumbnail" style="border: 0; border-right: medium solid #000000;"><img class="img-rounded" src="images/Mainack-Mondal.jpg">
                        <div class="caption">
                            <p>
                                <div><strong>Email: </strong><img src="images/mainack-new-add-kgp.png" border=0 alt="Mainack Mondal" style="width:80%;height:80%;">
								<br>
    							<a href="https://scholar.google.com/citations?user=AUllGoAAAAAJ&hl=en" target="_new">Google Scholar</a>
								<br><br>
						Department of Computer Science and Engineering, IIT Kharagpur, <br>
						Room : 316<br> 
						IIT Kharagpur, West Bengal, <br>
						PIN: 721302, India 

						</div>
                            </p>
                        </div>
                    </div>
				</div>

<!-- Bio Part-->
        		<div class="col-md-9" id="bio">
        			<div class=row>
        				<p  align="justify"> I am an Assistant Professor at <a href="http://cse.iitkgp.ac.in">Department of Computer Scinece and Engineering, IIT Kharagpur, India</a>. Previously, I was a postdoctoral researcher at <a href="https://tech.cornell.edu/">Cornell Tech</a> and a member of the <a href="https://www.dli.tech.cornell.edu/">Digital Life Initiative</a> where I worked with <a href="https://nissenbaum.tech.cornell.edu/">Prof. Helen Nissenbaum</a>. Prior to joinining Cornell Tech I spent a fantastic year as a postdoc at the <a href="https://www.cs.uchicago.edu/" target="_new">University of Chicago, Department of Computer Science</a>. There I was a member of <a href="https://super.cs.uchicago.edu/" target="_new">SUPERgroup</a> and worked with <a href="https://www.blaseur.com/" target="_new">Prof. Blase Ur</a>. I completed my Ph.D. in Computer Science on November 2017 at the <a target="_new" href="https://www.mpi-sws.org/">Max Planck Institute for Software systems</a> where I was advised by <a  target="_new" href="https://www.mpi-sws.org/~gummadi/">Prof. Krishna P. Gummadi.</a></p>

						<p align="justify">I am broadly interested about incorporating human factors in security and privacy, and consequently designing usable online services. My recent research focus is on developing systems to provide usable privacy and security mechanisms to online users while minimizing system abuse.</p>

						<p align="justify"><b><font size="2" color="#FF0000">I am looking for students who are interested in human aspects of privacy/security and like to tinker with systems. If  you are a student at IIT Kgp && if you feel strongly about making the digital world private and secure again do drop me a mail (or just drop by in my office). </font></b></p>


<!-- updates Part-->
        				<div class="col-md-12" id="updates">  
							<div class="row">
        						<h3 style ="border-bottom: solid #000;border-top: solid #fff;">Latest updates</h3>

	        								<div align="justify">
	        								July 2019: <b>Co-teaching two graduate level courses: Social Computing and Cryptography & Network Security.</b>.
	        								</div>

	        								<div align="justify">
	        								July 2019: <b>Joined <a href="http://cse.iitkgp.ac.in">Department of Computer Scinece and Engineering, IIT Kharagpur, India</a> as Assistant Professor.</b>.
	        								</div>
        						
	        								<div align="justify">
	        								March 2019: <b>Serving as a program committe member for <a href="http://privaci.info/ci_symposium/cfp.html">2nd Symposium on Applications of Contextual Integrity</a></b>.
	        								</div>

	        								<div align="justify">
	        								March 2019: <b>Serving as publications co-chair at <a href="https://www.comsnets.org/">COMSNETS 2020</a>.</b>
	        								</div>
	        								<div align="justify">
	        								March 2019: <b>Released a hatespeech dataset from our work <a href="https://github.com/Mainack/hatespeech-data-HT-2017/"  target="_new">here</a></b>.</b>
	        								</div>
							</div>
						</div>


<!-- projects Part-->
   	 					<div class="col-md-12" id="research">     
        					<div class="row">
        						<h3 style ="border-bottom: solid #000;border-top: solid #fff;">Research Interests</h3>
        						<p align="justify">I design, implement and analyze usable private and secure online systems. My work integrates security and privacy, human-computer interaction and systems research. My prominent ongoing research projects are as follows:<p>
        						<p></p>
	       								<font size="3" color="#C70039">Improving usability of retrospective access management in online archives</font> <sup><font color="blue">[PoPETS'19] [SOUPS'18]</font></sup> 
										<p align="justify">
										We are investigating the effectiveness of tools which enable users to retrospectively modify (delete/edit old content or retrospectively change the audience) their past content in online archives (like social media or cloud storage). Our final goal is to design new mechanisms and systems which will let online users better manage the security and privacy of their old content. 
										</p>
								<p></p>
	       								<font size="3" color="#C70039">Managing online data privacy and security via exposure</font> <sup><font color="blue"> [IJAESAM'17] [IC'17] [SOUPS'16] [USEC'14] [SOUPS'14] [CoNEXT'12] [EuroSys'12]</font></sup>
										<p align="justify">
										We developed the model of exposure control (controlling who actually views a piece of online content), an extension of existing access control (controlling who has access to the online content) for building more secure/private systems. We show that, in multiple real-world scenarios, exposure control enables us to better capture user intention and design more private and usable systems compare to the state of the art.
										</p>
								<p></p>
	       								<font size="3" color="#C70039">Limiting abuse (hatespeech) in online platforms</font> <sup><font color="blue" >[THAM'18] [HT'17] [ICWSM'16] [ICWSM'15] </font><a target="_new" href="https://github.com/Mainack/hatespeech-data-HT-2017/"><span class="badge">DATASET</span></a></sup>
										<p align="justify">
										We also investigated user behavior in online anonymous platforms. We identified that privacy and anonymity is a blessing to most of the user since they enable users to upheld free speech. However, a few users abuse the system under the veil of anonymity in the form of posting content like hatespeech. To that end, we work on developing techniques to detect and investigate hate speech in online platforms. <b>You can access our hatespeech dataset <a href="https://github.com/Mainack/hatespeech-data-HT-2017/"  target="_new">here</a></b>.
										</p>
									

							</div>
						</div>

<!-- Publications Part-->
   	 					<div class="col-md-12" id="publications">     
        					<div class="row">
  								<h3 style ="border-bottom: solid #000;border-top: solid #fff;margin-bottom: 0.5cm;">Publications</h3>
                                      	<h4 style ="font-color:#fff;">Refereed publications</h4>	
                                        	<ul>
                                        		<li id="pets19">
 	                                    			<b>Lethe: Conceal Content Deletion from Persistent Observers</b><br/>
	                                    			Mohsen Minaei, <u>Mainack Mondal</u>, Patrick Loiseau, Krishna Gummadi, and Aniket Kate.<br />
	                                    			In the <i>Proceedings of Privacy Enhancing Technologies Symposium (PoPETS)</i>, Stockholm, Sweden, July 2019.<br />
	                                    			<a href="#pets19" onclick="toggle('pets19abstract')"><span class="label label-success">ABSTRACT</span></a> <a target="_new" href="https://mainack.tech.cornell.edu/publications/lethe-intermittent-withdrawal-pets-2019.pdf"><span class="label label-success">PDF</span></a> <a target="_new" href="https://mainack.tech.cornell.edu/bibtex/lethe-intermittent-withdrawal-pets-2019.html"><span class="label label-success">BIBTEX</span></a><a target="_new" href="https://arxiv.org/abs/1710.11271">&nbsp<span class="label label-success">ArXiv (PRELIMINARY)</span></a></span></a>&nbsp<span class="label label-warning">JOURNAL</span>
                                          			<p></p>
      	                                    		<p align="justify" id="pets19abstract" style="display:none;">
    	                                    		<b>Abstract:</b> 
Most social platforms offer mechanisms allowing users to delete their posts, and a significant fraction of users exercise this right to be forgotten. However, ironically, users’ attempt to reduce attention to sensitive posts via deletion, in practice, attracts unwanted attention from stalkers specifically to those (deleted) posts. Thus, deletions may leave users more vulnerable to attacks on their privacy in general. Users hoping to make their posts forgotten face a “damned if I do, damned if I don’t” dilemma. Many are shifting towards ephemeral social platform like Snapchat, which will deprive us of important user-data archival. In the form of intermittent withdrawals, we present, Lethe, a novel solution to this problem of (really) forgetting the forgotten. If the next-generation social platforms are willing to give up the uninterrupted availability of non-deleted posts by a very small fraction, Lethe provides privacy to the deleted posts over long durations. In presence of Lethe, an adversarial observer becomes unsure if some posts are permanently deleted or just temporarily withdrawn by Lethe; at the same time, the adversarial observer is overwhelmed by a large number of falsely flagged undeleted posts. To demonstrate the feasibility and performance of Lethe, we analyze large-scale real data about users’ deletion over Twitter and thoroughly investigate how to choose time duration distributions for alternating between temporary withdrawals and resurrections of non-deleted posts. We find a favorable trade-off between privacy, availability and adversarial overhead in different settings for users exercising their right to delete. We show that, even against an ultimate adversary with an uninterrupted access to the entire platform, Lethe offers deletion privacy for up to 3 months from the time of deletion, while maintaining content availability as high as 95% and keeping the adversarial precision to 20%.
      	                                    		</br></br>
      	                                    		</p>
                                        		<p></p>
                                        		</li>


                                        		<li id="cisymp18">
 	                                    			<b>Enforcing Contextual Integrity With Exposure Control</b><br/>
	                                    			<u>Mainack Mondal</u> and Blase Ur.<br />
	                                    			In <i>the Symposium on Applications of Contextual Integrity</i>, Princeton, NJ, USA, September 2018.<br />
	                                    			<a href="#cisymp18" onclick="toggle('cisymp18abstract')"><span class="label label-success">ABSTRACT</span></a> <a target="_new" href="https://mainack.tech.cornell.edu/publications/exposure-ci-2018.pdf"><span class="label label-success">PDF</span></a> <a target="_new" href="https://mainack.tech.cornell.edu/bibtex/exposure-ci-2018.html"><span class="label label-success">BIBTEX</span></a></span></a>
                                          			<p></p>
      	                                    		<p align="justify" id="cisymp18abstract" style="display:none;">
    	                                    		<b>Abstract:</b> 
The normative model of contextual integrity (CI) equips individuals to reason about privacy requirements and violations in online systems. However, a subsequent step is the enforcement of CI in online systems via privacy-management mechanisms. In this work, we  rst investigate the suitability of access control, the dominant privacy management model in online platforms, in  lling this role. We argue that access control is insu cient for enforcing CI because it does not consider the set of expected recipients for a piece of content. To that end, we identify the privacy model of exposure control as an extension of access control to better enforce CI. We discuss the e ectiveness of exposure control in better enforcing CI and describe a generic prediction-based framework for controlling exposure in online systems.
      	                                    		</br></br>
      	                                    		</p>
                                        		<p></p>
                                        		</li>

                                        		<li id="soups18">
 	                                    			<b>Making Retrospective Data Management Usable </b><br/>
	                                    			Noah Hirsch, Chris Kanich, Mohammad Taha Khan, Xuefeng Liu, <u>Mainack Mondal</u>, Michael Tang, Christopher Tran, Blase Ur, William Wang, Günce Su Yılmaz, Elena Zheleva.<br />
	                                    			In <i>Proceedings of the 14th Symposium on Usable Privacy and Security (SOUPS'18)</i>, Baltimore, MD, USA, August 2018.<br />
	                                    			<a href="#soups18" onclick="toggle('soups18abstract')"><span class="label label-success">ABSTRACT</span></a> <a target="_new" href="https://mainack.tech.cornell.edu/publications/soups2018-retrospective-privacy.pdf"><span class="label label-success">PDF</span></a> <a target="_new" href="https://mainack.tech.cornell.edu/bibtex/soups2018-retrospective-privacy.html"><span class="label label-success">BIBTEX</span></a></span></a>&nbsp<span class="label label-warning">POSTER</span>
                                          			<p></p>
      	                                    		<p align="justify" id="soups18abstract" style="display:none;">
    	                                    		<b>Abstract:</b> 
Today, online archives like social media or cloud storage systems store personal data shared by billions of users. For many accounts, these archives accumulate data over multiple years. Recent work suggested that users feel the need for retrospectively managing security and privacy of this huge volume of content. However, there is also a scarcity of mechanisms and systems to help these users retrospectively manage their data. To that end, in this work we point out the need of creating usable retrospective data management mechanisms and outline our vision for a possible architecture to address this challenge.
      	                                    		</br></br>
      	                                    		</p>
                                        		<p></p>
                                        		</li>

                                        		<li id="tham18">
 	                                    			<b>Characterizing Usage of Explicit Hate Expressions in Social Media </b><br/>
	                                    			<u>Mainack Mondal</u>, Leandro Arau ́jo Silva, Denzil Correa and Fabr ́ıcio Benevenuto.<br />
	                                    			In <i>New Review of Hypermedia and Multimedia (THAM)</i>, vol. 24, no. 2, pp. 110-130, June 2018.<br />
	                                    			<a href="#tham18" onclick="toggle('tham18abstract')"><span class="label label-success">ABSTRACT </span></a>&nbsp<a target="_new" href="https://mainack.tech.cornell.edu/publications/hatespeech-tham-2018.preprint.pdf"><span class="label label-success">PDF [PREPRINT]</span></a>&nbsp<a target="_new" href="https://mainack.tech.cornell.edu/bibtex/hatespeech-tham-2018.html"><span class="label label-success">BIBTEX</span></a>&nbsp<span class="label label-warning">JOURNAL</span>&nbsp<a target="_new" href="https://github.com/Mainack/hatespeech-data-HT-2017/"><button class="label label-info">DATASET</button></a>
                                          			<p></p>
      	                                    		<p align="justify" id="tham18abstract" style="display:none;">
    	                                    		<b>Abstract:</b> Social media platforms provide an inexpensive communication medium that allows anyone to publish content and anyone interested in the content can obtain it. However, this same potential of social media provide space for discourses that are harmful to certain groups of people. Examples of these discourses include bullying, offensive content, and hate speech. Out of these discourses hate speech is rapidly recognized as a serious problem by authorities of many countries. In this paper, we provide the first of a kind systematic large-scale measurement and analysis study of explicit expressions of hate speech in online social media. We aim to understand the abundance of hate speech in online social media, the most common hate expressions, the effect of anonymity on hate speech, the sensitivity of hate speech and the most hated groups across regions. In order to achieve our objectives, we gather traces from two social media systems: Whisper and Twitter. We then develop and validate a methodology to identify hate speech on both of these systems. Our results identify hate speech forms and unveil a set of important patterns, providing not only a broader understanding of online hate speech, but also offering directions for detection and prevention approaches.
      	                                    		</br></br>
      	                                    		</p>
                                        		<p></p>
                                        		</li>

                                        		<li id="hilda18">
 	                                    			<b>Draining the Data Swamp: A Similarity-based Approach</b><br/>
	                                    			Will Brackenbury, Rui Liu, <u>Mainack Mondal</u>, Aaron Elmore, Blase Ur, Kyle Chard, Michael J. Franklin.<br />
	                                    			In <i>Proceedings of the Workshop on Human-In-the-Loop Data Analytics (HILDA)</i>, Houston, TX, June 2018.<br />
	                                    			<a href="#hilda18" onclick="toggle('hilda18abstract')"><span class="label label-success">ABSTRACT </span></a>&nbsp<a target="_new" href="https://mainack.tech.cornell.edu/publications/dataswamp-hilda18.pdf"><span class="label label-success">PDF</span></a> <a target="_new" href="https://mainack.tech.cornell.edu/bibtex/dataswamp-hilda18.html"><span class="label label-success">BIBTEX</span></a>
                                          			<p></p>
      	                                    		<p align="justify" id="hilda18abstract" style="display:none;">
    	                                    		<b>Abstract:</b> While hierarchical namespaces such as filesystems and repositories have long been used to organize data, the rapid increase in data production places increasing strain on users who wish to make use of the data. So called "data lakes" embrace the storage of data in its natural form, integrating and organizing in a Pay-as-you-go fashion. While this model defers the upfront cost of integration, the result is that data is unusable for discovery or analysis until it is processed. Thus, data scientists are forced to spend significant time and energy on mundane tasks such as data discovery, cleaning, integration, and management – when this is neglected, "data lakes" become "data swamps". Prior work suggests that pure computational methods for resolving issues with the data discovery and management components are insufficient. Here, we provide evidence to confirm this hypothesis, showing that methods such as automated file clustering are unable to extract the necessary features from repositories to provide useful information to end-user data scientists, or make effective data management decisions on their behalf. We argue that the combination of frameworks for specifying file similarity and human-in-the-loop interaction is needed to aid automated organization.We propose an initial step here, classifying several dimensions by which items may be considered similar: the data, its origin, and its current characteristics. We initially consider this model in the context of identifying data that can be integrated or managed collectively.We additionally explore how current methods can be used to automate decision making using real-world data repository and file systems, and suggest how an online user study could be developed to further validate this hypothesis.
      	                                    		</br></br>
      	                                    		</p>
                                        		<p></p>
                                        		</li>
                              	
                                        		<li id="IJAESAM17">
 	                                    			<b>Managing Longitudinal Exposure of Socially Shared Data on the Twitter Social Media</b><br/>
	                                    			<u>Mainack Mondal</u>, Johnnatan Messias, Saptarshi Ghosh, Krishna P Gummadi, Aniket Kate.<br />
	                                    			In <i>International Journal of Advances in Engineering Sciences and Applied Mathematics (IJAESAM)</i>, vol. 9, no. 4, pp. 238-257, December 2017.<br />
	                                    			<a href="#IJAESAM17" onclick="toggle('IJAESAM17abstract')"><span class="label label-success">ABSTRACT</span></a> <a target="_new" href="https://mainack.tech.cornell.edu/publications/longitudinal_exposure-IJAESAM17-preprint.pdf"><span class="label label-success">PDF (PREPRINT)</span></a> <a target="_new" href="https://mainack.tech.cornell.edu/bibtex/IJAESAM17-longitudinal-exposure.html"><span class="label label-success">BIBTEX</span></a>&nbsp<span class="label label-warning">JOURNAL</span>
                                          			<p></p>
      	                                    		<p align="justify" id="IJAESAM17abstract" style="display:none;">
    	                                    		<b>Abstract:</b> On most online social media sites today, user-generated data remains accessible to allowed viewers unless and until the data owner changes her privacy preferences. In this paper, we present a large-scale measurement study focused on understanding how users control the longitudinal exposure of their publicly shared data on social media sites. Our study, using data from Twitter, finds that a significant fraction of users withdraw a surprisingly large percentage of old publicly shared data---more than 28% of six-year old public posts (tweets) on Twitter are not accessible today. The inaccessible tweets are either selectively deleted by users or withdrawn by users when they delete or make their accounts private. We also found a significant problem with the current exposure control mechanisms -- even when a user deletes her tweets or her account, the current mechanisms leave traces of residual activity, i.e., tweets from it other users sent as replies to those deleted tweets or accounts still remain accessible. We show that using this residual information one can recover significant information about the deleted tweets or even characteristics of the deleted accounts. To the best of our knowledge, we are the first to study the information leakage resulting from residual activities of deleted tweets and accounts. Finally, we propose two exposure control mechanisms that eliminates information leakage via residual activities. One of our mechanisms optimize for allowing meaningful social interactions with user posts and another mechanism aims to control longitudinal exposure via anonymization . We discuss the merits and drawbacks of our proposed mechanisms compared to existing mechanisms.
      	                                    		</br></br>
      	                                    		</p>
                                        		<p></p>
                                        		</li>

                                        		<li id="ht17">
 	                                    			<b>A Measurement Study of Hate Speech in Social Media </b><br/>
	                                    			<u>Mainack Mondal</u>, Leandro Araújo Silva, Fabrício Benevenuto.<br />
	                                    			In <i>Proceedings of the 25th ACM Conference on Hypertext and Social Media (HT'17)</i>, Prague, Czech Republic, July 2017.<br/>
	                                    			<a href="#ht17" onclick="toggle('ht17abstract')"><span class="label label-success">ABSTRACT</span></a> <a target="_new" href="https://mainack.tech.cornell.edu/publications/hatespeech-ht-2017.pdf"><span class="label label-success">PDF</span></a> <a target="_new" href="https://mainack.tech.cornell.edu/bibtex/hatespeech-ht-2017.html"><span class="label label-success">BIBTEX</span></a>&nbsp<span class="label label-danger">TED NELSON AWARD NOMINEE</span>&nbsp<a target="_new" href="https://github.com/Mainack/hatespeech-data-HT-2017/"><button class="label label-info">DATASET</button></a>
                                          			<p></p>
      	                                    		<p align="justify" id="ht17abstract" style="display:none;">
    	                                    		<b>Abstract:</b> Social media platforms provide an inexpensive communication medium that allows anyone to quickly reach millions of users. Consequently, in these platforms anyone can publish content and anyone interested in the content can obtain it, representing a transformative revolution in our society. However, this same potential of social media systems brings together an important challenge---these systems provide space for discourses that are harmful to certain groups of people. This challenge manifests itself with a number of variations, including bullying, offensive content, and hate speech. Specifically, authorities of many countries today are rapidly recognizing hate speech as a serious problem, specially because it is hard to create barriers on the Internet to prevent the dissemination of hate across countries or minorities. In this paper, we provide the first of a kind systematic large scale measurement and analysis study of hate speech in online social media. We aim to understand the abundance of hate speech in online social media, the most common hate expressions, the effect of anonymity on hate speech and the most hated groups across regions. In order to achieve our objectives, we gather traces from two social media systems: Whisper and Twitter. We then develop and validate a methodology to identify hate speech on both of these systems. Our results identify hate speech forms and unveil a set of important patterns, providing not only a broader understanding of online hate speech, but also offering directions for detection and prevention approaches.
      	                                    		</br></br>
      	                                    		</p>
                                        		<p></p>
                                        		</li>


                                         		<li id="ieeeic17">
                                         			<!-- <b>Understanding and Specifying Social Access Control Lists <i>(<font color="#D92810">To appear</font>)</i></b><br /> -->
	                                     			<b>Longitudinal Privacy Management in Social Media: The Need for Better Controls </b><br/>
	                                     			<u>Mainack Mondal</u>, Johnnatan Messias, Saptarshi Ghosh, Krishna P. Gummadi and Aniket Kate.<br />
	                                     			In <i>IEEE Internet Computing</i>, vol. 21, no. 3, pp. 48-55, May-June 2017.<br />
	                                     			<a href="#ieeeic17" onclick="toggle('ieeeic17abstract')"><span class="label label-success">ABSTRACT</span></a> <a target="_new" href="https://mainack.tech.cornell.edu/publications/ieee-ic-2017-longitudinal-exposure_preprint.pdf"><span class="label label-success">PDF (PREPRINT)</span></a> <a target="_new" href="https://mainack.tech.cornell.edu/bibtex/ieee-ic-2017-longitudinal-exposure.html"><span class="label label-success">BIBTEX</span></a>&nbsp<span class="label label-warning">JOURNAL</span>
                                           			<p></p>
      	                                     		<p align="justify" id="ieeeic17abstract" style="display:none;">
    	                                     		<b>Abstract:</b> This large-scale measurement study of Twitter focuses on understanding how users control the longitudinal exposure of their publicly shared social data — that is, their tweets — and the limitations of currently used control mechanisms. Our study  nds that, while Twitter users widely employ longitudinal exposure control mechanisms, they face two fundamental problems. First, even when users delete their data or account, the current mechanisms leave signi cant traces of residual activity. Second, these mechanisms single out withdrawn tweets or accounts, attracting undesirable attention to them. To address both problems, an inactivity-based withdrawal scheme for improved longitudinal exposure control is explored.
      	                                     		</br></br>
      	                                     		</p>
                                         			<p></p>
                                         		</li>


                                        		<li id="soups16">
                                     		    	<!-- <b>Understanding and Specifying Social Access Control Lists <i>(<font color="#D92810">To appear</font>)</i></b><br /> -->
                                     				<b>Forgetting in Social Media: Understanding and Controlling Longitudinal Exposure of Socially Shared Data </b><br/>
                                     				<u>Mainack Mondal</u>, Johnnatan Messias, Saptarshi Ghosh, Krishna P. Gummadi and Aniket Kate.<br />
                                     				In <i>Proceedings of the 12th Symposium on Usable Privacy and Security (SOUPS'16)</i>, Denver, CO, USA, June 2016.<br />
                                     				<a href="#soups16" onclick="toggle('soups16abstract')"><span class="label label-success">ABSTRACT</span></a> <a target="_new" href="https://mainack.tech.cornell.edu/publications/soups2016-longitudinal-exposure.pdf"><span class="label label-success">PDF</span></a> <a target="_new" href="https://mainack.tech.cornell.edu/bibtex/soups2016-longitudinal-exposure.html"><span class="label label-success">BIBTEX</span></a>
                                     		      	<p></p>
                                     		      	<p align="justify" id="soups16abstract" style="display:none;">
                                     		    	<b>Abstract:</b> On most online social media sites today, user-generated data remains accessible to allowed viewers unless and until the data owner changes her privacy preferences. In this paper, we present a large-scale measurement study focussed on understanding how users control the longitudinal exposure of their publicly shared data on social media sites. Our study, using data from Twitter, finds that a significant fraction of users withdraw a surprisingly large percentage of old publicly shared data -- more than 28% of six-year old public posts (tweets) on Twitter are not accessible today. The inaccessible tweets are either selectively deleted by users or withdrawn by users when they delete or make their accounts private. We also found a significant problem with the current exposure control mechanisms – even when a user deletes her tweets or her account, the current mechanisms leave traces of residual activity, i.e., tweets from other users sent as replies to those deleted tweets or accounts still remain accessible. We show that using this residual information one can recover significant information about the deleted tweets or even characteristics of the deleted accounts. To the best of our knowledge, we are the first to study th information leakage resulting from residual activities of deleted tweets and accounts. Finally, we propose an exposure control mechanism that eliminates information leakage via residual activities, while still allowing meaningful social interactions with user posts. We discuss its merits and drawbacks compared to existing mechanisms.
                                     		      	</br></br>
                                     		      	</p>
                                     		    	<p></p>
                                        		</li>


                                        		<li id="icwsm16poster">
                                           		    <b>Analyzing the Targets of Hate in Online Social Media </b><br/>
	                                     		    Leandro Araújo Silva, <u>Mainack Mondal</u>, Denzil Correa, Fabrício Benevenuto and Ingmer Weber.<br />

	                                     		    In <i>Poster session, 10th International AAAI Conference on Weblogs and Social Media (ICWSM'16)</i>, Cologne, Germany, May 2016.<br />
	                                     		    <a href="#icwsm16poster" onclick="toggle('icwsm16posterabstract')"><span class="label label-success">ABSTRACT</span></a> <a target="_new" href="https://mainack.tech.cornell.edu/publications/2016_hatespeech_icwsm_poster.pdf"><span class="label label-success">PDF</span></a>&nbsp<span class="label label-warning">POSTER</span>&nbsp<a target="_new" href="https://github.com/Mainack/hatespeech-data-HT-2017/"><button class="label label-info">DATASET</button></a>
                                           		    <p></p>
      	                                     		    <p align="justify" id="icwsm16posterabstract" style="display:none;">
    	                                     		    <b>Abstract:</b> Social media systems allow Internet users a congenial platform to freely express their thoughts and opinions. Although this property represents incredible and unique communication opportunities, it also brings along important challenges. Online hate speech is an archetypal example of such challenges. Despite its magnitude and scale, there is a significant gap in understanding the nature of hate speech on social media. In this paper, we provide the first of a kind systematic large scale measurement study of the main targets of hate speech in online social media. To do that, we gather traces from two social media systems: Whisper and Twitter. We then develop and validate a methodology to identify hate speech on both these systems. Our results identify online hate speech forms and offer a broader understanding of the phenomenon, providing directions for prevention and detection approaches.
      	                                     		    </br></br>
      	                                     		    </p>
                                         		    	<p></p>
                                        		</li>
  
  
                                        		<li id="icwsm15">
	                                     		    <b>The Many Shades of Anonymity: Characterizing Anonymous Social Media Content </b> <br />
	                                     		    Denzil Correa, Leandro Araújo Silva, <u>Mainack Mondal</u>, Fabrício Benevenuto and Krishna P. Gummadi.<br />
	                                     		    In <i>Proceedings of The 9th International AAAI Conference on Weblogs and Social Media (ICWSM'15)</i>, Oxford, UK, May 2015.<br />
	                                     		    <a href="#icwsm15" onclick="toggle('icwsm15abstract')"><span class="label label-success">ABSTRACT</span></a> <a target="_new" href="https://mainack.tech.cornell.edu/publications/icwsm2015-whisper.pdf"><span class="label label-success">PDF</span></a> <a target="_new" href="https://mainack.tech.cornell.edu/bibtex/icwsm2015-whisper.html"><span class="label label-success">BIBTEX</span></a>
                                           		    <p></p>
      	                                     		    <p align="justify" id="icwsm15abstract" style="display:none;">
    	                                     		    <b>Abstract:</b> Recently, there has been a significant increase in the popularity of anonymous social media sites like Whisper and Secret. Unlike traditional social media sites like Facebook and Twitter, posts on anonymous social media sites are not associated with well-defined user identities or profiles. In this study, our goals are two-fold: (i) to understand the nature (sensitivity, types) of content posted on anonymous social media sites and (ii) to investigate the differences between content posted on anonymous and non-anonymous social media sites like Twitter. To this end, we gather and analyze extensive content traces from Whisper (anonymous) and Twitter (non-anonymous) social media sites. We introduce the notion of <i>anonymity sensitivity </i> of a social media post, which captures the extent to which users think the post should be anonymous. We also propose a human annotator based methodology to measure the same for Whisper and Twitter posts. Our analysis reveals that anonymity sensitivity of most whispers (unlike tweets) is not binary. Instead, most whispers exhibit <i> many shades </i> or different levels of anonymity.  We also find that the linguistic differences between whispers and tweets are so significant that we could train automated classifiers to distinguish between them with reasonable accuracy. Our findings shed light on human behavior in anonymous media systems that lack the notion of an identity and they have important implications for the future designs of such systems.
                                     		        	 </br></br>
      													 </p>
      													 <p></p>
      											</li>


                                        		<li id="soups14">
 	                                    			<!-- <b>Understanding and Specifying Social Access Control Lists <i>(<font color="#D92810">To appear</font>)</i></b><br /> -->
 	                                    			<b>Understanding and Specifying Social Access Control Lists </b><br />
	      											<u>Mainack Mondal</u>, Yabing Liu, Bimal Viswanath, Krishna P. Gummadi and Alan Mislove.<br />
	      											In <i>Proceedings of the 10th Symposium on Usable Privacy and Security (SOUPS'14)</i>, Menlo Park, CA, USA, July 2014.<br />
	      											<a href="#soups14" onclick="toggle('soups14abstract')"><span class="label label-success">ABSTRACT</span></a> <a target="_new" href="https://mainack.tech.cornell.edu/publications/soups2014-final27.pdf"><span class="label label-success">PDF</span></a> <a target="_new" href="https://mainack.tech.cornell.edu/bibtex/soups2014-final27.html"><span class="label label-success">BIBTEX</span></a>&nbsp<span class="label label-danger">DISTINGUISHED PAPER AWARD</span>
 	                                    			<p></p>
      	      										<p align="justify" id="soups14abstract" style="display:none;">
    	      										<b>Abstract:</b> Online social network (OSN) users upload millions of pieces of content  to share with others every day. While a significant portion of this content is benign  (and is typically shared with all friends or all OSN users), there are certain pieces of  content that are highly privacy sensitive. Sharing such sensitive content raises  significant privacy concerns for users, and it becomes important for the user to  protect this content from being exposed to the wrong audience. Today, most OSN services provide fine-grained mechanisms for specifying social access control lists (social ACLs, or SACLs), allowing users to restrict their sensitive content to a select subset of their friends. However, it remains unclear how these SACL mechanisms are used today. To design better privacy management tools for users, we need to first understand the usage and complexity of SACLs specified by users. <br></br> In this paper, we present the first large-scale study of fine-grained privacy preferences of over 1,000 users on Facebook, providing us with the first ground-truth information on how users specify SACLs on a social networking service. Overall, we find that a surprisingly large fraction (17.6 %) of content is shared with SACLs. However, we also find that the SACL membership shows little correlation with either profile information or social network links; as a result, it is difficult to predict the subset of a user's friends likely to appear in a SACL. On the flip side, we find that SACLs are often re-used, suggesting that simply making recent SACLs available to users is likely to significantly reduce the burden of privacy management on users.      	<
    	      										</br></br>
      												</p>
    												<p></p>
                                        		</li>

                                        		<li id="usec14">
   													<b>Beyond Access Control: Managing Online Privacy via Exposure</b><br />
	   												<u>Mainack Mondal</u>, Peter Druschel, Krishna P. Gummadi. and Alan Mislove.<br />
	   												In <i>Proceedings of the  Workshop on Usable Security (USEC'14)</i>, San Diego, CA, USA, February 2014.<br />
	   												<a href="#usec14" onclick="toggle('usec14abstract')"><span class="label label-success">ABSTRACT</span></a> <a target="_new" href="https://mainack.tech.cornell.edu/publications/usec2014-final46.pdf"><span class="label label-success">PDF</span></a> <a target="_new" href="https://mainack.tech.cornell.edu/bibtex/usec2014-final46.html"><span class="label label-success">BIBTEX</span></a>
         											<p></p>
      	   											<p align="justify" id="usec14abstract" style="display:none;">
    	   											<b>Abstract:</b> We posit that access control, the dominant model for modeling and managing privacy in today's online world, is fundamentally inadequate. First, with access control, users must <i>a priori</i> specify precisely who can or cannot access information by enumerating users, groups, or roles---a task that is difficult to get right. Second, access control fails to separate who <i> can </i> access information from who actually <i>does</i>, because it ignores the difficulty of <i> finding</i> information. Third, access control does not capture if and how a person who has access to some information redistributes that information. Fourth, access control fails to account for information that can be inferred from other, public information.
		   											<br>
		   											We present <i>exposure</i> as an alternate model for information privacy; exposure captures the set of people expected to learn an item of information eventually. We believe the model takes an important step towards enabling users to model and control their privacy effectively.  
      	   											</br></br>
      	   											</p>
       												<p></p>
                                        		</li>

                                        		<li id="cscw14">
       												 <b>Deep Twitter Diving: Exploring Topical Groups in Microblogs at Scale</b><br />
       												Parantapa Bhattacharya, Saptarshi Ghosh, Juhi Kulshrestha, <u>Mainack Mondal</u>, Muhammad Bilal Zafar, Niloy Ganguly, and Krishna P. Gummadi.<br />
       												In <i> Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work and Social Computing (CSCW'14)</i>, Baltimore, MD, USA, February 2014.<br />
       												<a href="#cscw14" onclick="toggle('cscw14abstract')"><span class="label label-success">ABSTRACT</span></a> <a target="_new" href="https://mainack.tech.cornell.edu/publications/topicgroups_twitter.pdf"><span class="label label-success">PDF</span></a> <a target="_new" href="https://mainack.tech.cornell.edu/bibtex/topicgroups_twitter.html"><span class="label label-success">BIBTEX</span></a>
       												<p></p>
       												<p align="justify" id="cscw14abstract" style="display:none;">
       												<b>Abstract:</b> We present a semantic methodology to identify <i>topical groups</i> in Twitter on a large number of topics, each consisting of users who are experts on or interested in a specific topic. Early studies investigating the nature of Twitter suggest that it is a social media platform consisting of a relatively small section of elite users, producing information on a few popular topics such as media, politics, and music, and the general population consuming it. We show that this characterization ignores a rich set of highly specialized topics, ranging from geology, neurology, to astrophysics and karate -- each being discussed by their own topical groups. We present a detailed characterization of these topical groups based on their network structures and tweeting behaviors. Analyzing these groups on the backdrop of the common identity and bond theory in social sciences shows that these groups exhibit characteristics of topical-identity based groups, rather than social-bond based ones.
       												</br></br>
       												 </p>
       												 <p></p>
                                        		</li>
  
                                        		<li id="conext12">
       												 <b>Defending against large-scale crawls in online social networks</b><br />
       												 <u>Mainack Mondal</u>, Bimal Viswanath,  Allen Clement, Peter Druschel, Krishna P. Gummadi, Alan Mislove and Ansley Post.<br />
       												 In <i> Proceedings of the 8th International Conference on emerging Networking EXperiments and Technologies (CoNEXT'12)</i>, Nice, France, December 2012.<br />
       												 <a href="#conext12" onclick="toggle('conext12abstract')"><span class="label label-success">ABSTRACT</span></a> <a target="_new" href="https://mainack.tech.cornell.edu/publications/Genie-CoNEXT.pdf"><span class="label label-success">PDF</span></a> <a target="_new" href="https://mainack.tech.cornell.edu/bibtex/Genie-CoNEXT.html"><span class="label label-success">BIBTEX</span></a>&nbsp<a  target="_new" href="https://mainack.tech.cornell.edu/Slides/Genie-CoNEXT.ppt"><span class="label label-warning">https://mainack.tech.cornell.edu/Slides/</span></a>

       												 <p></p>
       												 <p align="justify" id="conext12abstract" style="display:none;">
       												 <b>Abstract:</b> Thwarting large-scale crawls of user profiles in online social networks (OSNs) like Facebook and Renren is in the interest of both the users and the operators of these sites. OSN users wish to maintain control over their personal information, and OSN operators wish to protect their business assets and reputation. Existing rate-limiting techniques are ineffective against crawlers with many accounts, be they fake accounts (also known as Sybils) or compromised accounts of real users obtained on the black market.
       												 <br>
       												 We propose Genie, a system that can be deployed by OSN operators to defend against crawlers in large-scale OSNs. Genie exploits the fact that the browsing patterns of honest users and crawlers are very different: even a crawler with access to many accounts needs to make many more profile views per account than an honest user, and view profiles of users that are more distant in the social network. Experiments using real-world data gathered from a popular OSN show that Genie frustrates large-scale crawling while rarely impacting honest users; the few honest users who are affected can recover easily by adding a few friend links.      	
       												 </br></br>
       												 </p>
        											<p></p>
                                        		</li>

                                        		<li id="www12demo">
       												<b>Simplifying Friendlist Management</b> (Demo Paper)<br />
       												Yabing Liu, Bimal Viswanath, <u>Mainack Mondal</u>, Krishna P. Gummadi, and Alan Mislove.<br />
       												In <i>Proceedings of the 21st International World Wide Web Conference (WWW'12)</i>, Lyon, France, April 2012.<br />
       												<a href="#www12demo" onclick="toggle('www12demoabstract')"><span class="label label-success">ABSTRACT</span></a> <a target="_new" href="https://mainack.tech.cornell.edu/publications/Friendlist-WWW-Demo.pdf"><span class="label label-success">PDF</span></a> <a target="_new" href="https://mainack.tech.cornell.edu/bibtex/Friendlist-WWW-Demo.html"><span class="label label-success">BIBTEX</span></a>

       												<p></p>
       												<p align="justify" id="www12demoabstract" style="display:none;">
       												<b>Abstract:</b> Online social networks like Facebook allow users to connect, communicate, and share content. The popularity of these services has lead to an {\\em information overload} for their users; the task of simply keeping track of different interactions has become daunting. To reduce this burden, sites like Facebook allows the user to group friends into specific lists, known as {\\em friendlists}, aggregating the interactions and content from all friends in each friendlist. While this approach greatly reduces the burden on the user, it still forces the user to create and populate the friendlists themselves and, worse, makes the user responsible for maintaining the membership of their friendlists over time.
       												<br>
       												We show that friendlists often have a strong correspondence to the structure of the social network, implying that friendlists may be automatically inferred by leveraging the social network structure. We present a demonstration of Friendlist Manager, a Facebook application that proposes friendlists to the user based on the structure of their local social network, allows the user to tweak the proposed friendlists, and then automatically creates the friendlists for the user.
       												</br></br>
       												</p>
        											<p></p>
                                        		</li>

                                        		<li id="eurosys12">
       												<b>Canal: Scaling social network-based Sybil tolerance schemes</b><br />
       												Bimal Viswanath, <u>Mainack Mondal</u>, Krishna P. Gummadi, Alan Mislove and Ansley Post.<br />
       												In <i>Proceedings of the 7th European Conference on Computer Systems (EuroSys’12)</i>, Bern, Switzerland, April 2012.<br />
       												<a href="#eurosys12" onclick="toggle('eurosys12abstract')"><span class="label label-success">ABSTRACT</span></a> <a target="_new" href="https://mainack.tech.cornell.edu/publications/Canal-EuroSys.pdf"><span class="label label-success">PDF</span></a> <a target="_new" href="https://mainack.tech.cornell.edu/bibtex/Canal-EuroSys.html"><span class="label label-success">BIBTEX</span></a>
       												<p></p>
       												<p align="justify" id="eurosys12abstract" style="display:none;">
       												<b>Abstract:</b> There has been a flurry of research on leveraging social networks to defend against multiple identity, or Sybil, attacks. A series of recent works does not try to explicitly identify Sybil identities and, instead, bounds the impact that Sybil identities can have. We call these approaches <i> Sybil tolerance</i>; they have shown to be effective in applications including reputation systems, spam protection, online auctions, and content rating systems. All of these approaches use a social network as a credit network, rendering multiple identities ineffective to an attacker without a commensurate increase in social links to honest users (which are assumed to be hard to obtain). Unfortunately, a hurdle to practical adoption is that Sybil tolerance relies on computationally expensive network analysis, thereby limiting widespread deployment.		
       												<br>
       												To address this problem, we first demonstrate that despite their differences, all proposed Sybil tolerance systems work by conducting payments over credit networks. These payments require max flow computations on a social network graph, and lead to poor scalability. We then present <i>Canal</i>, a system that uses landmark routing-based techniques to efficiently approximate credit payments over large networks. Through an evaluation on real-world data, we show that Canal provides up to a three-order-of-magnitude speedup while maintaining safety and accuracy, even when applied to social networks with millions of nodes and hundreds of millions of edges. Finally, we demonstrate that Canal can be easily plugged into existing Sybil tolerance schemes, enabling them to be deployed in an online fashion in real-world systems.
       												</br></br>
       												</p>
       												<p></p>
                                        		</li>

                                        		<li id="sigcomm11poster">
       												<b>Limiting Large-scale Crawls of Social Networking Sites </b><br />
       												<u>Mainack Mondal</u>, Bimal Viswanath,  Allen Clement, Peter Druschel, Krishna P. Gummadi, Alan Mislove and Ansley Post.<br />
       												In <i> Poster session, Annual Conference of the ACM Special Interest Group on Data Communication (SIGCOMM'11)</i>,Toronto, Canada, August 2011.<br />
       												<a href="#sigcomm11poster" onclick="toggle('sigcomm11posterabstract')"><span class="label label-success">ABSTRACT</span></a> <a target="_new" href="https://mainack.tech.cornell.edu/publications/2011_genie_sigcomm_poster.pdf"><span class="label label-success">PDF</span></a>&nbsp<span class="label label-warning">POSTER</span>&nbsp<span class="label label-danger">SIGCOMM'11 STUDENT RESEARCH COMPETITION FINALIST</span>
       												<p></p>
       												<p align="justify" id="sigcomm11posterabstract" style="display:none;">
       												<b>Abstract:</b> Online social networking sites (OSNs) like Facebook and Orkut contain personal data of millions of users. Many OSNs view this data as a valuable asset that is at the core of their business model. Both OSN users and OSNs have strong incentives to restrict large scale crawls of this data. OSN users want to protect their privacy and OSNs their business interest. Traditional defenses against crawlers involve rate-limiting browsing activity per user account. These defense schemes, however, are vulnerable to Sybil attacks, where a crawler creates a large number of fake user accounts. In this paper, we propose Genie, a system that can be deployed by OSN operators to defend against Sybil crawlers. Genie is based on a simple yet powerful insight: <i>the social network itself can be leveraged to defend against Sybil crawlers.</i> We first present Genie's design and then discuss how Genie can limit crawlers while allowing browsing of user profiles by normal users.
       												</br></br>
       												</p>
       												<p></p>
                                        		</li>

                                        		<li id="wissec10">
           											<b>TweLEX: A tweaked version of the LEX stream cipher</b><br />
           											<u>Mainack Mondal</u>, Avik Chakraborty, Nilanjan Dutta, Debdeep Mukhopadhyay.<br />
           											In <i> 5th Benelux Workshop on Information and System Security, (WISSec’10)</i>, Nijmegen, the Netherlands, November 2010.<br />
           											<a href="#wissec10" onclick="toggle('wissec10abstract')"><span class="label label-success">ABSTRACT</span></a> <a target="_new" href="https://mainack.tech.cornell.edu/publications/2010_wissec_tweLEX_short_talk.pdf"><span class="label label-success">PDF</span></a> <a  target="_new" href="https://mainack.tech.cornell.edu/Slides/2010_wissec_tweLEX_short_talk_presentation.pdf"><span class="label label-warning">https://mainack.tech.cornell.edu/Slides/</span></a> 
           											<p></p>
           											<p align="justify" id="wissec10abstract" style="display:none;">
           											<b>Abstract:</b> LEX is a stream cipher proposed by Alex Biryukov. It was selected to phase 3 of the eSTREAM competition. LEX is based on the Advanced Encryption Standard (AES) block cipher and uses a methodology called Leak Extraction, proposed by Biryukov himself. However Dunkelman and Keller show that a key recovery attack exists against LEX. Their attack requires 2<sup>36.3</sup> bytes of keystream produced by the same key and works with a time complexity of 2<sup>112</sup> operations. In this work we explore LEX further and have shown that under the assumption of a related key model we can obtain 24 secret state bytes with a time complexity of 296 and a data complexity of 2<sup>54.3</sup>. Subsequently, we introduce a tweaked version of LEX, called TweLEX, which is shown to resist all known attacks against LEX. Though the throughput of TweLEX is half of LEX, it is still 1.25 times faster than AES, the underlying block cipher. This work attempts to revive the principle of leak extraction as a simple and elegant method to design stream ciphers.
           											</br></br>
           											</p>
           											<p></p>
                                        		</li>
                   									 <li id="vlsid10">
               										 <b>Pinpointing Cache Timing Attacks on AES</b><br />
               										 Chester Rebeiro, <u>Mainack Mondal</u>, Debdeep Mukhopadhyay.<br />
               										 In <i> 23rd International Conference on VLSI design and 9th International Conference on Embedded Systems (VLSID'10)</i>,Bangalore, India, January 2010.<br />
               										 <a href="#vlsid10" onclick="toggle('vlsid10abstract')"><span class="label label-success">ABSTRACT</span></a> <a target="_new" href="https://mainack.tech.cornell.edu/publications/2010_aescache-10.pdf"><span class="label label-success">PDF</span></a>
               										 <p></p>
               										 <p align="justify" id="vlsid10abstract" style="display:none;">
               										 <b>Abstract:</b> The paper analyzes cache based timing attacks on optimized codes for Advanced Encryption Standard (AES). The work justifies that timing based cache attacks create hits in the first and second rounds of AES, in a manner that the timing variations leak information of the key. To the best of our knowledge, the paper justifies for the first time that these attacks are unable to force hits in the third round and concludes that a similar third round cache timing attack does not work. The paper experimentally verifies that protecting only the first two AES rounds thwarts cache based timing attacks.
               										</br></br>
               										</p>
               										<p></p>
                   								</li>
                                        	</ul>
                                      	<h4 style ="font-color:#fff;">Non-refereed publications</h4>
                                        	<ul>

                                        		<li id="sideways17">
    												<b>Double-edged Swords: The Good and the Bad of Privacy and Anonymity in Social Media</b> (Invited talk abstract)<br />
													<u>Mainack Mondal</u><br />
													In <i> Proceedings of the 3rd International Workshop on Social Media World Sensors (SIDEWAYS'17)</i>, Prague, Czech Republic, July 2017.<br/>
													<a target="_new" href="https://mainack.tech.cornell.edu/publications/goodbad-2017-sideways.pdf"><span class="label label-success">PDF</span></a> <a target="_new" href="https://mainack.tech.cornell.edu/bibtex/goodbad-2017-sideways.html"><span class="label label-success">BIBTEX</span></a>
      												</p>
    												<p></p>
    											</li>

                                        		<li id="comsnets12">
    												<b>Exploring the design space of social network-based Sybil defenses</b> (Invited paper)<br />
													Bimal Viswanath, <u>Mainack Mondal</u>,  Allen Clement, Peter Druschel, Krishna P. Gummadi, Alan Mislove and Ansley Post.<br />
													In <i> Proceedings of the 4th International Conference on Communication Systems and Networks (COMSNETS'12)</i>, Bangalore, India, January 2012.<br />
													<a href="#comsnets12" onclick="toggle('comsnets12abstract')"><span class="label label-success">ABSTRACT</span></a> <a target="_new" href="https://mainack.tech.cornell.edu/publications/sybil_defense_comsnets_2012.pdf"><span class="label label-success">PDF</span></a> <a target="_new" href="https://mainack.tech.cornell.edu/bibtex/Sybil-COMSNETS.html"><span class="label label-success">BIBTEX</span></a>
													<p></p>
      												<p align="justify" id="comsnets12abstract" style="display:none;">
    												<b>Abstract:</b> Recently, there has been significant research interest in leveraging social networks to defend against Sybil attacks. While much of this work may appear similar at first glance, existing social network-based Sybil defense schemes can be divided into two categories: <i>Sybil detection</i> and <i> Sybil tolerance </i>. These two categories of systems both leverage global properties of the underlying social graph, but they rely on different assumptions and provide different guarantees: Sybil detection schemes are application-independent and rely only on the graph structure to identify Sybil identities, while Sybil tolerance schemes rely on application-specific information and leverage the graph structure and transaction history to bound the leverage an attacker can gain from using multiple identities. In this paper, we take a closer look at the design goals, models, assumptions, guarantees, and limitations of both categories of social network-based Sybil defense systems.
      												</br></br>
      												</p>
    												<p></p>
    											</li>

     											<li id="mpitr11">
    												    <b>Defending against large-scale crawls in online social networks</b><br />
    													<u>Mainack Mondal</u>, Bimal Viswanath,  Allen Clement, Peter Druschel, Krishna P. Gummadi, Alan Mislove and Ansley Post.<br />
    													<i> MPI-SWS Technical Report 2011-006</i>, MPI-SWS, November 2011.<br />
    													<a href="#mpitr11" onclick="toggle('mpitr11abstract')"><span class="label label-success">ABSTRACT</span></a> <a  target="_new" href="http://www.mpi-sws.org/tr/2011-006.pdf"><span class="label label-success">PDF</span></a> <a target="_new" href="https://mainack.tech.cornell.edu/bibtex/Genie-TR.html"><span class="label label-success">BIBTEX</span></a>
    												    <p></p>
    												    <p align="justify" id="mpitr11abstract" style="display:none;">
    												    <b>Abstract:</b>  Thwarting large-scale crawls of user profiles in online social networks (OSNs) like Facebook and Renren is in the interest of both the users and the operators of these sites. OSN users wish to maintain control over their personal information, and OSN operators wish to protect their business assets and reputation. Existing rate-limiting techniques are ineffective against crawlers with many accounts, be they fake accounts (also known as Sybils) or compromised accounts of real users obtained on the black market.
    												    <br>
    												     We propose Genie, a system that can be deployed by OSN operators to defend against crawlers in large-scale OSNs. Genie exploits the fact that the browsing patterns of honest users and crawlers are very different: even a crawler with access to many accounts needs to make many more profile views per account than an honest user, and view profiles of users that are more distant in the social network. Experiments using real-world data gathered from a popular OSN show that Genie frustrates large-scale crawling while rarely impacting honest users; the few honest users who are affected can recover easily by adding a few friend links.
    													</br></br>
    													</p>
    													 <p></p>
     											</li>

    										</ul>
							</div>
						</div>

<!-- Softwares Part-->
        				<div class="col-md-12" id="softwaresdatasets">  
							<div class="row">
        						<h3 style ="border-bottom: solid #000;border-top: solid #fff;">Our Systems/Datasets</h3>
	        						<p align="justify">A common theme of our work is to collect real world data from deployed systems and analyze this data to identify and address privacy, security or accountability issues in those systems. Consequently, we created some online systems as part of our research to help social network users better understand and manage their data privacy. Please find below a list of such system and datasets from our work:</p>

        						<h4 style ="border-bottom: solid #000;border-top: solid #fff;">Systems developed</h4>

	        						<ul>
	        							<li>
	        								<p align="justify">
	        								<a href="http://twitter-app.mpi-sws.org/footprint/"  target="_new"> Check Your Secondary Digital Footprint on Twitter:</a>  In Twitter, people may converse with you by mentioning your name in their tweets. These conversations constitute your secondary digital footprint. Secondary digital footprints are not created or controlled by you. However, they can still leak your personal information. Our Twitter application aims to help you check what information others leak about you on Twitter (You will need a Twitter account to use it ). 
	        								</p>
	        								<p></p>
	        							</li>
	        							<li>
	        								<p align="justify">
	        								<a href="http://friendlist-manager.mpi-sws.org/"  target="_new">Friendlist Manager:</a> <a href="https://www.facebook.com/notes/facebook/improved-friend-lists/10150278932602131"  target="_new">Friendlists in Facebook</a> are a great way to share your content with the people you intend to. But  they are a huge pain to create and update. Our Facebook application was designed to facilitate and simplify management of your friendlists. Unfortunately the new version of Facebook API do not allow developers to fetch the data the app needed to use,  consequently the app is not live any more. You can check the functions of this (now discontinued) <a href="http://friendlist-manager.mpi-sws.org/"  target="_new"> app here</a>. <!--You will need a Facebook account to use it.-->
	        								</p>
	        								<p></p>
	        							</li>
	        							<li>
	        								<p align="justify">
	        								<a href="http://privacy-iq.mpi-sws.org/"  target="_new">Privacy IQ</a>: Privacy IQ is a quiz that measures both your understanding of how  privacy works on Facebook and your knowledge of your own privacy settings. However due to the change in Facebook API this app too is not live any more. <!--You can check the functions of this (now discontinued) <a href="http://privacy-iq.mpi-sws.org/" target="_new">app here</a>.--> 
	        								<!--You will need a Facebook account to use it.-->
	        								</p>
	        							</li>
	        						</ul>
							</div>
        						<h4 style ="border-bottom: solid #000;border-top: solid #fff;">Datasets from our work</h4>
	        						<ul>
	        							<li>
	        								<p align="justify">
	        								<a href="https://github.com/Mainack/hatespeech-data-HT-2017/"  target="_new"> Hatespeech data collected from Twitter and Whisper:</a>  This <a href="https://github.com/Mainack/hatespeech-data-HT-2017/"  target="_new">dataset</a> contains 20,705 tweets and 7,604 whispers which contain hatespeech. To know more about our methodology please check our <a href="https://mainack.tech.cornell.edu/publications/hatespeech-ht-2017.pdf"  target="_new">HyperText 2017 publication</a> .
	        								</p>
	        								<p></p>
	        							</li>

	        						</ul>
							</div>
						</div>



<!-- Teaching Part-->
        				<div class="col-md-12" id="teaching">  
        					<div class="row">
		        				<h3 style ="border-bottom: solid #000;border-top: solid #fff;">Teaching</h3>
	        							<ul>

	        								<li>
	        									<p align="justify">
	        									<a href=""  target="_new">Cryptography and Network Security (Autumn 2019)</a> <sup><font color="#C70039"><b>[CO-TEACHING] [IIT Kgp]</b></font></sup>
	        									</p>
	        								</li>

	        								<li>
	        									<p align="justify">
	        									<a href=""  target="_new">Social Computing (Autumn 2019)</a> <sup><font color="#C70039"><b>[CO-TEACHING] [IIT Kgp]</b></font></sup>
	        									</p>
	        								</li>

	        								<li>
	        									<p align="justify">
	        									<a href="https://super.cs.uchicago.edu/usable18/"  target="_new">Usable Security and Privacy (Spring 2018)</a> <sup><font color="#C70039"><b>[CO-TEACHING] [UChicago]</b></font></sup>
	        									</p>
	        								</li>

	        								<li>
	        									<p align="justify">
	        									<a href="https://super.cs.uchicago.edu/topics18/"  target="_new">Topics in Computer Security: Data-Driven Security and Privacy (Winter 2018)</a> <sup><font color="#C70039"><b>[CO-TEACHING] [UChicago]</b></font></sup>
	        									</p>
	        								</li>

	        								<li>
	        									<p align="justify">
	        									<a href="http://courses.mpi-sws.org/sma-ss16/"  target="_new">Social Media Analysis (Summer 2016)</a><sup><font color="#C70039"><b>[TA] [MPI-SWS/Saarland University]</b></font></sup>
	        									</p>
	        								</li>
	        								<li>
	        									<p align="justify">
	        									<a href="https://www.sps.cs.uni-saarland.de/teaching/14WS/core-security/index.html"  target="_new">Security (Winter 2014-15)</a> <sup><font color="#C70039"><b>[TA] [Saarland University]</b></font></sup>
	        									<!--You will need a Facebook account to use it.-->
	        									</p>
	        								</li>
	        								<li>
	        									<p align="justify">
	        									<a href="http://courses.mpi-sws.org/sscs-ss13/"  target="_new">Readings in Social Computing Systems (Summer 2013)</a> <sup><font color="#C70039"><b>[TA] [MPI-SWS/Saarland University]</b></font></sup>
	        									<!--You will need a Facebook account to use it.-->
	        									</p>
	        								</li>
	        							</ul>

							</div>
						</div>

<!-- professional services Part-->
        				<div class="col-md-12" id="services">  
							<div class="row">
        						<h3 style ="border-bottom: solid #000;border-top: solid #fff;">Professional Services</h3>
        						
	        								<div align="justify">
	        								<b><font color="#C70039">PC Member:</font></b> COMSNETS'20,  CoDS-COMAD (YRS)'20, ALW'19, CI'19, CHI'19 Workshop, DIS PWiP'19, COMSNETS SNW'2019, CoDS-COMAD (YRS)'19, TRAC'18.
	        								</div>

	        								<div align="justify">
	        								<b><font color="#C70039">Reviewer (Journal):</font></b> ACM ToN, IEEE IC, ACM TOIT, ACM TWEB, IEEE TPDS, IEEE TDSC, Elsevier JPDC.
	        								</div>

	        								<div align="justify">
	        								<b><font color="#C70039">Reviewer (Conference):</font></b> ACM CSCW, ACM CHI, ACM AsiaCCS, ICWSM, WWW, WebSci.
	        								</div>

							</div>
						</div>


	<!--<div id="footer">Updated May, 2014</div>-->
				<!--<div class="col-md-9" id="end">
					<div class="row"> 
						<div id="footer" align="right">
							<p align="right">Updated <?php echo date ("F, Y", filemtime("tStamp.txt"));?></p>
			</div>
		</div>
    </div>-->
    				</div>
    			</div>
    		</div>
    	</div>
    </div>
    
    <!-- /.container -->

	
    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="bootstrap/js/jquery/1.11.0/jquery.min.js"></script>
    <script src="bootstrap/js/bootstrap.min.js"></script>
    <!--<script type="text/javascript">function getModDate(){var e=document.lastModified;var t=e.split(" ")[0].split("/");var n=new Array;n[0]="January";n[1]="February";n[2]="March";n[3]="April";n[4]="May";n[5]="June";n[6]="July";n[7]="August";n[8]="September";n[9]="October";n[10]="November";n[11]="December";var r=n[t[0]*1 - 1];var i=t[2];var s="Updated "+r+", "+i;document.getElementById("footer").innerHTML=s;}; getModDate();</script>-->
  </body>
</html>
